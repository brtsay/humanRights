{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Human Rights Tweets with Vowpal Wabbit\n",
    "\n",
    "I tried to help out on a project where the researchers were looking to map the source of tweets abouts human rights in the United States. The training data they used and which was provided to me came from posts made by human rights activists/organizations on Twitter. Both the training/test sets and unlabeled tweets were provided to me with no alteration/supplementation on my part. The following is an attempt to get at the problem using [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) (VW) version 8.1.1. I had a few issues trying to install the [Python package](https://pypi.python.org/pypi/vowpalwabbit) via `pip`, so I decided to install the command-line version from the default Ubuntu repos and use this opportunity to become more familiar with Bash scripting.\n",
    "\n",
    "In a previous attempt, I used Facebook's [fastText](https://fasttext.cc/) to classify the tweets. Since the default training/test data format that fastText accepts is similar to VW's, I will slightly modify it to VW's preferred format. The added benefit is that I have already preprocessed the fastText data to lower-case all words and to remove all URL links and punctuation.\n",
    "\n",
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 99035\n",
      "Number of human rights tweets in train set: 34893\n",
      "Number of valid examples: 24808\n",
      "Number of human rights tweets in valid set: 8771\n",
      "Number of test examples: 31114\n",
      "Number of human rights tweets in test set: 11018\n"
     ]
    }
   ],
   "source": [
    "labeled=(\"train\" \"valid\" \"test\")\n",
    "for type in \"${labeled[@]}\";\n",
    "do\n",
    "    echo Number of $type examples: $(wc -l < fast$type.txt)\n",
    "    echo Number of human rights tweets in $type set: $(grep -c \"__label__hr\" fast$type.txt)\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, I was provided with around 150,000 labeled examples, with human rights tweets being outnumbered by a ratio of about 2:1. \n",
    "\n",
    "This is what the data looks like (warning, some tweets may be explicit...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__hr get the real facts on medical malpractice 2015 update   \n",
      "__label__hr meet flavia shes on a mission to restore peoples right to clean water  \n",
      "__label__hr usa senates resistance to reupping patriotact shows momentum against mass surveillance   \n",
      "__label__hr were celebrating 10 years of promoting amp protecting human rights in afghanistan learn more about our work  \n",
      "__label__hr majorityspeaks youve been quoted in our storify mymadre link roundup raising our voices and creating solutions  \n",
      "__label__hr in palestine dignity and violence madre friend noam chomsky on the continuing crisis  \n",
      "__label__hr rt richardengel un over 191000 killed in syria conflict \n",
      "__label__hr rt bbcbreaking nelson mandela south africas first black president dies aged 95  \n",
      "__label__hr china opens fire on peaceful tibetan protesters injuring 10 sad un navi pillays parting speech ignored china  \n",
      "__label__hr the 2014 international labor rights defenders askgeorge cwaunion cgt at ilrf awards what an amazing night  \n",
      "__label__hr breaking appeals court halts execution of russell bucklew  via nbcnews missouri deathpenalty \n",
      "__label__hr you still have time on this givingtuesday please help us advance humanrights for the most underserved  thank you \n",
      "__label__hr rt laurapitter omarkhadr finally released on bail today after nearly 13 years of unlawful detention abuse  http \n",
      "__label__hr rt financevtraffik following guardian reports on forcedlabour in fishing industry ilrf show steps companies can take to address this  \n",
      "__label__hr new syria report war crimes against besieged civilians  \n",
      "__label__nonhr loooong day i officially quit this day  \n",
      "__label__nonhr cnn beginning of the end  been a good ride humans just another failed genetic mutation now  \n",
      "__label__nonhr real_liam_payne just a week till home time cant wait to see my favourite person in the world me obviously \n",
      "__label__nonhr pnkeyyy di nga eh may quiz pa ko every report dn sa asian at exam thesis pa ngyon lang to haha  \n",
      "__label__nonhr sacramento ca transportation job delivery driver pt northgate blvd at genuine parts company napa  \n",
      "__label__nonhr dangg mann i hope my boys ova at mgsc keeping they head up  shouldve known that was coming smdh \n",
      "__label__nonhr gregr1077 my cousin darthkripple is working on an itunes version of our podcast ill let you know when it goes up  \n",
      "__label__nonhr victoriaokane question do u think some1 should get fired for not telling obama a terrorist organization isis was taking over iraq \n",
      "__label__nonhr keatoncobble hahaha i would totally give it to you if could \n",
      "__label__nonhr thesgxo today i got a phone call saying you were a lesbian \n",
      "__label__nonhr noo nooo es amor lo que t√∫ sientes se llama obsesion \n",
      "__label__nonhr alexbootyfleek real as fuck  \n",
      "__label__nonhr cant wait for dance moms hyped \n",
      "__label__nonhr evaanderson97 yea oops \n",
      "__label__nonhr rypitme ducatimotor ducatiusa did you pick \n"
     ]
    }
   ],
   "source": [
    "grep  '^__label__hr.*' fasttrain.txt | shuf -n 15\n",
    "grep  '^__label__nonhr.*' fasttrain.txt | shuf -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first part of every line is the label in \"\\_\\_label\\_\\_\" format followed by the actual text, which is the default format fastText accepts. VW's default format is similar. The first part of every line is \"1+ |\" for a positive example and \"-1 |\" for a negative example. Let's change the format to one that VW accepts and make some additional changes.\n",
    "\n",
    "Also notice that some of these posts may not necessarily be related to human rights, even though they are labeled as such. For example, the tweet \"majorityspeaks youve been quoted in our storify mymadre link roundup raising our voices and creating solutions \" by itself would probably not be thought of as human rights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRF: 0.96789\n",
      "weight: 2\n",
      "--passes 10 --loss_function logistic --ngram w1 --skips w2 --learning_rate .80000000000000000000\n"
     ]
    }
   ],
   "source": [
    "cat vw/vw_bestparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to running this notebook, I ran random search over a set of parameters, including label weights, that I will describe in more detail later. The reason I bring it up now is because the version of VW I use does not allow us to pass label weight as an option, but must instead be included in the actual training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 2|w ff cleanclothes stitchistas usleap 10campaign globalexchange \t|l len:62\n",
      "-1 |w sexualglf whats a boyfriend if hes not your bestfriend \t|l len:56\n",
      "+1 2|w check out the awesome things our partners are up to this week un  unpfii \t|l len:74\n",
      "+1 2|w ff data2x which is launching in a big way today \t|l len:49\n",
      "-1 |w please dont leave your sense in september god bless you as you comply \t|l len:71\n",
      "+1 2|w with 1200 deaths and no end in sight please help our partners provide aid and critical care to civilians in gaza  \t|l len:115\n",
      "-1 |w alexxxisann yeah too bad kanyes a douchebag  \t|l len:46\n",
      "-1 |w same old sgit just a different day \t|l len:36\n",
      "+1 2|w rt ajam after four years of syrias war no end in sight  \t|l len:57\n",
      "-1 |w korinichole15 last name nigga first name thug im just a g like that \t|l len:69\n"
     ]
    }
   ],
   "source": [
    "while read line; do\n",
    "    if [[ $line == weight* ]]; then\n",
    "        weight=$(echo $line | grep -o -P '(?<=weight: )[0-9]+';)\n",
    "    fi\n",
    "done < vw/vw_bestparams\n",
    "\n",
    "cat fasttrain.txt | awk '{sub(\"__label__hr\", \"+1 '$weight'|w\"); sub(\"__label__nonhr\", \"-1 |w\"); print $0}' | shuf > vw/traintmpW\n",
    "cat vw/traintmpW | sed 's:.*|w::'| awk '$0=\"|l len:\"length($0)' > vw/traintmpL\n",
    "paste vw/traintmpW vw/traintmpL > vw/vwtrain.txt\n",
    "rm vw/traintmp*\n",
    "\n",
    "head vw/vwtrain.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read in the \"best\" weight from our random search. VW allows uses to assign importance weights to individual training examples. This is generally useful for when the trainng set is unbalanced between different classes, which our training set is. I simply applied a uniform weight (greater than 1) weight to all examples of human rights tweets, since it was the smaller class. The weight of 2 was found via random search, meaning that tweets labeled as being human rights-related are considered 2 times as important (since we keep our non-human rights tweets at weight 1).\n",
    "\n",
    "We then use awk to replace the fastText labels with labels that VW would understand e.g. \"\\_\\_label\\_\\_nonhr\" becomes \"-1 |w \". The VW label format for binary classification has positive labels be \"+1\" and negative labels be \"-1\" with a pipe | separating the label from the actual features. We add a \"w\" after the pipe to tell VW that the actual text will be considered to be in the \"w\" namespace. In VW, features can be split into multiple namespaces, and different actions can be applied to different namespaces. We will add another namespace in the next step.\n",
    "\n",
    "In the other namespace, which we call \"l\", we add the length of the tweet (after it has been preprocessed to remove links and punctuation) as another feature for our classifier. The guess would be that perhaps human rights tweets are less likely to be very short. To get the length, we first use sed to delete the label and then use awk to count the length of the post, saving the output into a separate file. We then paste our \"w\" and \"l\" namespaces together, deleting the temporary files we took them from.\n",
    "\n",
    "The data is shuffled becauase VW learns in an online fashion, one example at a time (the version I'm using does not seem to accept the minibatch option). The data was originally organized with all the positive examples first and the negative examples afterwards, which would have hampered online learning.\n",
    "\n",
    "We format the validation and test sets together, since they do not need a weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=(\"valid\" \"test\")\n",
    "for data in \"${files[@]}\";\n",
    "do\n",
    "    cat fast${data}.txt | awk '{sub(\"__label__hr\", \"+1 |w\"); sub(\"__label__nonhr\", \"-1 |w\"); print $0}'| shuf > vw/${data}tmpW\n",
    "    cat vw/${data}tmpW | sed 's:.*|w::'| awk '$0=\"|l len:\"length($0)' > vw/${data}tmpL\n",
    "    paste vw/${data}tmpW vw/${data}tmpL > vw/vw${data}.txt\n",
    "    rm vw/${data}tmp*\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I had ran random search using the script `vw_rsearch.sh`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./vw_rsearch.sh 100 false false vw/vwmodel vw/vwresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is telling VW to run random search for 100 iterations, not to force L1 regularization, not to supplement the labeled data (more on this later), what to name the model file, and what to name random search results file. It's not run in the notebook because while VW is fast, it would still take a while. \n",
    "\n",
    "The `bestresult.sh` script goes through the output of random search (`vw/vwresults`) and finds the entry that maximizes the metric of interest (`PRF`, or the F1 score), and writes it out to a file (`vw/vw_bestparams`). Metrics were found with the [perf](http://osmot.cs.cornell.edu/kddcup/software.html) software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRF: 0.96789\n",
      "weight: 2\n",
      "--passes 10 --loss_function logistic --ngram w1 --skips w2 --learning_rate .80000000000000000000\n"
     ]
    }
   ],
   "source": [
    "./bestresult.sh PRF 1 vw/vwresults vw/vw_bestparams\n",
    "cat vw/vw_bestparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The passes is the number of epochs i.e. how many times VW will go over the data. While it's currently set at 10, VW by default engages in early stopping, so it may not actually go through all 10 passes. Since this is a binary classification problem, I conducted random search with either logistic or hinge loss, with the \"best results\" coming from using logistic loss. Random search also determined that it was optimal to use unigrams over bi-, tri-, etc. grams. It's possible that for the \"average\" human rights tweet, surrounding word contexts don't actually matter; what determines human rights content is the presence of just a few keywords. When using n-grams, we can also have the n-grams skip over a number of words rather than just including concurrent words. However, since I am using unigrams, this option does nothing. It was not in the \"best parameters\", but random search also included specifications that included interactions between the \"w\" (the words) and \"l\" (length of tweet) namespaces as a feature.\n",
    "\n",
    "We see that the model with the highest F1 score is actually quite similar to VW's defaults. The only thing that changes is the number of passes through the data, the loss function, and the learning rate. There was no regularization, n-grams, or use of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--passes 10 --loss_function logistic --ngram w1 --skips w2 --learning_rate .80000000000000000000\n",
      "Generating 1-grams for w namespaces.\n",
      "Generating 2-skips for w namespaces.\n",
      "final_regressor = vw/vw.model\n",
      "Num weight bits = 24\n",
      "learning rate = 0.8\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = vw/vwtrain.txt.cache\n",
      "Reading datafile = vw/vwtrain.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0   1.0000  -1.0000        8\n",
      "1.000000 1.000000            2            2.0  -1.0000   1.0000       11\n",
      "0.500000 0.000000            4            4.0   1.0000   1.0000       12\n",
      "0.500000 0.500000            8            8.0  -1.0000  -1.0000        9\n",
      "0.500000 0.500000           16           16.0  -1.0000   1.0000        7\n",
      "0.375000 0.250000           32           32.0   1.0000  -1.0000       24\n",
      "0.265625 0.156250           64           64.0   1.0000  -1.0000       15\n",
      "0.218750 0.171875          128          128.0  -1.0000  -1.0000        4\n",
      "0.156250 0.093750          256          256.0  -1.0000  -1.0000        7\n",
      "0.146484 0.136719          512          512.0   1.0000   1.0000       22\n",
      "0.125000 0.103516         1024         1024.0  -1.0000  -1.0000       10\n",
      "0.103516 0.082031         2048         2048.0   1.0000   1.0000       15\n",
      "0.082275 0.061035         4096         4096.0   1.0000   1.0000       19\n",
      "0.064941 0.047607         8192         8192.0  -1.0000  -1.0000       11\n",
      "0.053406 0.041870        16384        16384.0  -1.0000  -1.0000       29\n",
      "0.045837 0.038269        32768        32768.0  -1.0000  -1.0000        8\n",
      "0.038559 0.031281        65536        65536.0  -1.0000  -1.0000       12\n",
      "0.039692 0.039692       131072       131072.0  -1.0000  -1.0000       11 h\n",
      "0.034575 0.029458       262144       262144.0  -1.0000  -1.0000        4 h\n",
      "0.031124 0.027673       524288       524288.0   1.0000   1.0000       10 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 89132\n",
      "passes used = 10\n",
      "weighted example sum = 891320.000000\n",
      "weighted label sum = -263020.000000\n",
      "average loss = 0.026457 h\n",
      "best constant = -0.608266\n",
      "best constant's loss = 0.648953\n",
      "total feature number = 11909650\n"
     ]
    }
   ],
   "source": [
    "while read line; do\n",
    "    if [[ $line == --* ]]; then\n",
    "        params=$line\n",
    "    fi\n",
    "done < vw/vw_bestparams\n",
    "echo $params\n",
    "vw --binary vw/vwtrain.txt -c -k -f vw/vw.model -b 24 $params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the \"best parameters\" and trained a new model using those parameters. It would have been more efficient to combine the performance evaluation with random search such that the \"best VW model was saved\" without having to retrain a model with the found best parameters. However, since VW is so fast, I just did it this way.\n",
    "\n",
    "By default, VW will generate its own holdout set from the training data you give it and evaluate on that. We see that this classifier achieved around 97% accuracy on that set. What about on labeled data it hasn't seen before?\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for w namespaces.\n",
      "Generating 2-skips for w namespaces.\n",
      "only testing\n",
      "raw predictions = vw/vwvalid_rawpred.txt\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = vw/vwvalid.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   1.0000   1.0000       16\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       13\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000        4\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000        4\n",
      "0.000000 0.000000           16           16.0   1.0000   1.0000       13\n",
      "0.000000 0.000000           32           32.0  -1.0000  -1.0000        3\n",
      "0.015625 0.031250           64           64.0  -1.0000  -1.0000       11\n",
      "0.023438 0.031250          128          128.0  -1.0000   1.0000       21\n",
      "0.023438 0.023438          256          256.0   1.0000   1.0000       17\n",
      "0.017578 0.011719          512          512.0  -1.0000  -1.0000        8\n",
      "0.014648 0.011719         1024         1024.0  -1.0000  -1.0000       13\n",
      "0.020996 0.027344         2048         2048.0  -1.0000  -1.0000        8\n",
      "0.020508 0.020020         4096         4096.0   1.0000   1.0000        7\n",
      "0.019775 0.019043         8192         8192.0  -1.0000  -1.0000        4\n",
      "0.023865 0.027954        16384        16384.0  -1.0000  -1.0000       24\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 24808\n",
      "passes used = 1\n",
      "weighted example sum = 24808.000000\n",
      "weighted label sum = -7266.000000\n",
      "average loss = 0.022856\n",
      "best constant = -0.292889\n",
      "best constant's loss = 0.914216\n",
      "total feature number = 331651\n"
     ]
    }
   ],
   "source": [
    "vw --binary -t -i vw/vw.model -r vw/vwvalid_rawpred.txt vw/vwvalid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually does slightly better on the unseen data, though not by much, still getting around 97% accuracy. What about other measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC    0.97714   pred_thresh  0.000000\n",
      "PRE    0.97731   pred_thresh  0.000000\n",
      "REC    0.95759   pred_thresh  0.000000\n",
      "PRF    0.96735   pred_thresh  0.000000\n"
     ]
    }
   ],
   "source": [
    "cut -d' ' -f1 vw/vwvalid.txt | paste - vw/vwvalid_rawpred.txt | perf.linux/perf -t 0 -PRE -REC -PRF -ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the [perf](http://osmot.cs.cornell.edu/kddcup/software.html) software to calculate our metrics. The precision (PRE) is high and the recall (REC) is slightly lower. \n",
    "\n",
    "The unlabeled text was provided to me as a .csv file with other tweet metadata, so I need to extract just the text and do all the lower-casing, URL and punctuation removing steps on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for w namespaces.\n",
      "Generating 2-skips for w namespaces.\n",
      "only testing\n",
      "predictions = vw/vwunlabeled_pred.txt\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = vw/vwfirst200k.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  unknown  -1.0000        3\n",
      "0.000000 0.000000            2            2.0  unknown  -1.0000        9\n",
      "0.000000 0.000000            4            4.0  unknown  -1.0000        7\n",
      "0.000000 0.000000            8            8.0  unknown  -1.0000       13\n",
      "0.000000 0.000000           16           16.0  unknown  -1.0000       14\n",
      "0.000000 0.000000           32           32.0  unknown  -1.0000        5\n",
      "0.000000 0.000000           64           64.0  unknown  -1.0000       21\n",
      "0.000000 0.000000          128          128.0  unknown  -1.0000        5\n",
      "0.000000 0.000000          256          256.0  unknown  -1.0000       27\n",
      "0.000000 0.000000          512          512.0  unknown  -1.0000        6\n",
      "0.000000 0.000000         1024         1024.0  unknown  -1.0000       13\n",
      "0.000000 0.000000         2048         2048.0  unknown  -1.0000       26\n",
      "0.000000 0.000000         4096         4096.0  unknown  -1.0000       10\n",
      "0.000000 0.000000         8192         8192.0  unknown  -1.0000       10\n",
      "0.000000 0.000000        16384        16384.0  unknown  -1.0000       27\n",
      "0.000000 0.000000        32768        32768.0  unknown  -1.0000       20\n",
      "0.000000 0.000000        65536        65536.0  unknown  -1.0000        9\n",
      "0.000000 0.000000       131072       131072.0  unknown  -1.0000        2\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 211180\n",
      "passes used = 1\n",
      "weighted example sum = 211180.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.000000\n",
      "total feature number = 2147677\n"
     ]
    }
   ],
   "source": [
    "cat first200k.csv | awk -F \"\\\"*,\\\"*\" '{print $12}' | sed 's/http[^ ]*//g' | tr -d '[:punct:]' | awk '{print \"|w \"tolower($0)}' | awk '{print($0)\" |l len:\" length($0)-2}' > vw/vwfirst200k.txt\n",
    "vw --binary -t -i vw/vw.model -p vw/vwunlabeled_pred.txt vw/vwfirst200k.txt\n",
    "paste vw/vwunlabeled_pred.txt vw/vwfirst200k.txt > vw/vwlabeled200k.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look through some of the predictions to see what the classifier did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted human rights tweets: 1617\n",
      "Number of total tweets: 211180\n",
      "\n",
      "Predicted as human rights\n",
      "1\t|w new faces of country thanks 93q  stafford centre  |l len:50\n",
      "1\t|w lopezemmanuel47 rollingfermin aguilascibaenas aguilapica luichysanchez no es en ninguna manera un insulto  barbarazo es ba√±o de pueblo |l len:135\n",
      "1\t|w cc allahpundit rt neiltyson cfchabris thanks sure |l len:50\n",
      "1\t|w facts only rt lolofbabyy ‚Äúuptownraised selling drugs gt going to college‚Äùüòí |l len:75\n",
      "1\t|w happy national podcast day nowgolistentomyadvertisers |l len:54\n",
      "1\t|w gun confiscated at bwi airport a pennsylvania man was arrested by maryland transportation authority police monday‚Ä¶  |l len:116\n",
      "1\t|w justicemercyinternational annual gala learning about what this great org does for people living in‚Ä¶  |l len:101\n",
      "1\t|w we are not for sale coreybbrooks on support for jimoberweis brucerauner  |l len:73\n",
      "1\t|w khamotabanerjee bibekdebroy tathagata2 dds ans to sikkito peepool over momwaste bengal will send 10 sputniks to pulutosponsorsaradha |l len:133\n",
      "1\t|w ‚Äúcloudflare inside shellshock how hackers are using it to exploit systems  via cloudflare‚Äù  excellent |l len:102\n",
      "1\t|w mfw wandows support hasnt replied to any of my tweets in months  |l len:65\n",
      "1\t|w firefighters at mhs respond to call  photo by jacob stalnaker mhsbeat  |l len:71\n",
      "1\t|w kimblake hudsonalpha researchers join worlds largest genetic study of bipolar disorder and schizophrenia  |l len:106\n",
      "1\t|w gabbiebrown yes route91harvest sent us a deniel email a little disappointed seeing weve been promoting the concert since they announced |l len:136\n",
      "1\t|w they went to rescue a dog hiding in the bush but found a big surprise hopeforpaws via upworthy  |l len:96\n",
      "1\t|w still confused why the youth of thus nation doesnt watch news |l len:62\n",
      "1\t|w a healthier tomorrow for more texas families is within our reach see abbott‚Äôs plan¬† |l len:84\n",
      "1\t|w china blocks instagram |l len:23\n",
      "1\t|w we need a group of teacher activists we must change the paradigm reginald jonessawyer jr ltyes mathmisplacement siliconvalleycf |l len:128\n",
      "1\t|w onceyougozachh you support gay rights so you must be gay |l len:57\n",
      "\n",
      "Predicted as not human rights\n",
      "-1\t|w ‚Äúinitiai nothing kills you like your mind‚Äù |l len:43\n",
      "-1\t|w jus saw mr sharp |l len:17\n",
      "-1\t|w  |l len:1\n",
      "-1\t|w sweetnonnie thanks much |l len:24\n",
      "-1\t|w kylehhailey otw14 was awesome  my first oow14 experience was much enriched by it so much great content thanks for organising it |l len:128\n",
      "-1\t|w that baby way to crunk |l len:23\n",
      "-1\t|w homw |l len:5\n",
      "-1\t|w damduckieee theyre real by benefit üòä |l len:37\n",
      "-1\t|w ecichanski that booty tho üôåüôåüôåüôåüôå |l len:32\n",
      "-1\t|w student worship hillcrestna begins  630 tomnight amp there will b one way 2 get in  one way 2 worship one way 2 access god intrigued |l len:133\n",
      "-1\t|w kateymarie thank god üôèüôèüôè |l len:25\n",
      "-1\t|w i love you |l len:11\n",
      "-1\t|w wonderful thank you again disneylandtoday hello earl of sandwich will open at 8am tomorrow |l len:91\n",
      "-1\t|w all im trynna do is make money |l len:31\n",
      "-1\t|w amiasaechell  |l len:14\n",
      "-1\t|w 1918champs true isnt it 7 x sox |l len:32\n",
      "-1\t|w aleealeman üôå dios dtbm üôå |l len:25\n",
      "-1\t|w babe¬ø  |l len:7\n",
      "-1\t|w posting picture of a black child saying i want one we r people |l len:63\n",
      "-1\t|w a sacremento nigga use to send the dope to me  youngstown my niggas sending smoke to me |l len:88\n"
     ]
    }
   ],
   "source": [
    "echo Number of predicted human rights tweets: $(grep '^1.*' vw/vwlabeled200k.txt | wc -l)\n",
    "echo Number of total tweets: $(wc -l < vw/vwlabeled200k.txt)\n",
    "printf \"\\n\"\n",
    "echo Predicted as human rights\n",
    "grep '^1.*' vw/vwlabeled200k.txt | shuf -n 20\n",
    "printf \"\\n\"\n",
    "echo Predicted as not human rights\n",
    "grep '^-1.*' vw/vwlabeled200k.txt | shuf -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through this small sample, it seems to generate a lot of false positives. After reading other tweets labeled as positive, it seems unlikely that the validation set precision was achieved here. It predicts that around .007% of the tweets are about human rights, which sounds plausible. What features did the classifier pick up on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw/vw-varinfo2 vw/vwtrain.txt -f vw/vwfeat.model -b 24 --loss_function logistic --learning_rate .8 > vw/features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the Python script [vw-varinfo2](https://github.com/arielf/weight-loss/blob/master/vw-varinfo2) to map the VW features back to interpretable words. I write out the parameters instead of feeding it a variable because it seems to have some trouble interpreting the `--passes` argument, so I just left it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureName     \t   HashVal\tMinVal\tMaxVal\tWeight\tRelScore\n",
      "Constant        \t  11650396\t1.00\t1.00\t-2.65\t -78.74\n",
      "w^shit          \t  13240385\t1.00\t1.00\t-2.62\t -77.82\n",
      "w^lol           \t  11494464\t1.00\t1.00\t-2.58\t -76.55\n",
      "w^fuck          \t  14898728\t1.00\t1.00\t-2.44\t -72.50\n",
      "w^fucking       \t  15120391\t1.00\t1.00\t-2.07\t -61.51\n",
      "w^lmao          \t  11217848\t1.00\t1.00\t-2.02\t -59.84\n",
      "w^idk           \t  11175912\t1.00\t1.00\t-1.90\t -56.45\n",
      "w^ass           \t   7056324\t1.00\t1.00\t-1.84\t -54.59\n",
      "w^funny         \t   2739481\t1.00\t1.00\t-1.83\t -54.34\n",
      "w^damn          \t  16768282\t1.00\t1.00\t-1.79\t -53.14\n",
      "w^bae           \t   2060971\t1.00\t1.00\t-1.78\t -52.96\n",
      "w^bro           \t   1718365\t1.00\t1.00\t-1.78\t -52.84\n",
      "w^game          \t   3665051\t1.00\t1.00\t-1.78\t -52.82\n",
      "w^im            \t  12491456\t1.00\t1.00\t-1.76\t -52.10\n",
      "w^oomf          \t   8191242\t1.00\t1.00\t-1.75\t -51.93\n",
      "w^donniewahlberg\t   5160006\t1.00\t1.00\t-1.74\t -51.79\n",
      "w^royals        \t  15825115\t1.00\t1.00\t-1.74\t -51.72\n",
      "w^annoying      \t   4399400\t1.00\t1.00\t-1.74\t -51.50\n",
      "w^rn            \t   8567985\t1.00\t1.00\t-1.73\t -51.31\n",
      "w^trash         \t   1874150\t1.00\t1.00\t-1.72\t -50.92\n",
      "w^guys          \t   3493851\t1.00\t1.00\t-1.71\t -50.78\n",
      "w^bruh          \t  11694126\t1.00\t1.00\t-1.68\t -49.86\n",
      "w^okay          \t   6889796\t1.00\t1.00\t-1.68\t -49.80\n",
      "w^hahaha        \t   7971613\t1.00\t1.00\t-1.67\t -49.61\n",
      "w^dope          \t  15980340\t1.00\t1.00\t-1.66\t -49.38\n",
      "w^smh           \t   7310166\t1.00\t1.00\t-1.66\t -49.14\n",
      "w^ima           \t   1966272\t1.00\t1.00\t-1.64\t -48.82\n",
      "w^nigga         \t   6388448\t1.00\t1.00\t-1.64\t -48.74\n",
      "w^me            \t   3799792\t1.00\t1.00\t-1.63\t -48.31\n",
      "w^bye           \t  13427699\t1.00\t1.00\t-1.62\t -48.04\n",
      "w^beer          \t   2377482\t1.00\t1.00\t-1.60\t -47.46\n",
      "w^yea           \t  11543119\t1.00\t1.00\t-1.56\t -46.43\n",
      "w^god           \t   2792509\t1.00\t1.00\t-1.55\t -45.94\n",
      "w^fav           \t   8797072\t1.00\t1.00\t-1.54\t -45.66\n",
      "w^nasty         \t   2330065\t1.00\t1.00\t-1.54\t -45.64\n",
      "w^playoff       \t    194800\t1.00\t1.00\t-1.53\t -45.45\n",
      "w^bitch         \t  13404399\t1.00\t1.00\t-1.52\t -45.27\n",
      "w^gonna         \t    934287\t1.00\t1.00\t-1.52\t -45.01\n",
      "w^wanna         \t   6625948\t1.00\t1.00\t-1.51\t -44.92\n",
      "w^lmfao         \t   9144904\t1.00\t1.00\t-1.51\t -44.70\n",
      "w^bed           \t   6188537\t1.00\t1.00\t-1.50\t -44.47\n",
      "w^af            \t   7450626\t1.00\t1.00\t-1.50\t -44.42\n",
      "w^tho           \t   4154126\t1.00\t1.00\t-1.49\t -44.20\n",
      "w^kidding       \t  14861508\t1.00\t1.00\t-1.48\t -44.05\n",
      "w^music         \t  10520813\t1.00\t1.00\t-1.48\t -44.03\n",
      "w^blakeshelton  \t   5932118\t1.00\t1.00\t-1.48\t -43.86\n",
      "w^u             \t   4190030\t1.00\t1.00\t-1.47\t -43.74\n",
      "w^hair          \t   4084369\t1.00\t1.00\t-1.47\t -43.66\n",
      "w^homecoming    \t  13164622\t1.00\t1.00\t-1.47\t -43.59\n",
      "w^unwatch       \t   6693375\t1.00\t1.00\t1.54\t  45.74\n",
      "w^gm            \t  16274076\t1.00\t1.00\t1.54\t  45.78\n",
      "w^prison        \t  15126285\t1.00\t1.00\t1.55\t  46.02\n",
      "w^international \t   9392983\t1.00\t1.00\t1.55\t  46.09\n",
      "w^uncat         \t  11885197\t1.00\t1.00\t1.57\t  46.60\n",
      "w^transgender   \t   3395411\t1.00\t1.00\t1.58\t  46.86\n",
      "w^blacklivesmatter\t  10940446\t1.00\t1.00\t1.60\t  47.38\n",
      "w^madre         \t   8036123\t1.00\t1.00\t1.60\t  47.41\n",
      "w^deathpenalty  \t   6225399\t1.00\t1.00\t1.60\t  47.52\n",
      "w^scotusmarriage\t  10814067\t1.00\t1.00\t1.60\t  47.62\n",
      "w^china         \t   6780032\t1.00\t1.00\t1.61\t  47.70\n",
      "w^sudan         \t  14486707\t1.00\t1.00\t1.62\t  48.02\n",
      "w^hillelneuer   \t   4330580\t1.00\t1.00\t1.62\t  48.19\n",
      "w^mekong        \t  15738601\t1.00\t1.00\t1.64\t  48.66\n",
      "w^arabia        \t   6245626\t1.00\t1.00\t1.65\t  48.93\n",
      "w^rights365     \t  10724794\t1.00\t1.00\t1.65\t  49.01\n",
      "w^trial         \t  11305875\t1.00\t1.00\t1.65\t  49.09\n",
      "w^iran          \t   8720818\t1.00\t1.00\t1.66\t  49.27\n",
      "w^torturereport \t  11973552\t1.00\t1.00\t1.66\t  49.28\n",
      "w^bangladesh    \t  14223056\t1.00\t1.00\t1.66\t  49.31\n",
      "w^oil           \t   3043835\t1.00\t1.00\t1.68\t  49.91\n",
      "w^russia        \t    850733\t1.00\t1.00\t1.68\t  49.92\n",
      "w^egypt         \t   1435846\t1.00\t1.00\t1.69\t  50.13\n",
      "w^womensrights  \t   7695761\t1.00\t1.00\t1.70\t  50.43\n",
      "w^amnesty       \t    571042\t1.00\t1.00\t1.70\t  50.53\n",
      "w^crc           \t  14986588\t1.00\t1.00\t1.71\t  50.71\n",
      "w^bahrain       \t   3360666\t1.00\t1.00\t1.72\t  50.97\n",
      "w^syria         \t  11337633\t1.00\t1.00\t1.72\t  51.15\n",
      "w^sharing       \t  12015507\t1.00\t1.00\t1.72\t  51.19\n",
      "w^fda           \t   2923984\t1.00\t1.00\t1.73\t  51.40\n",
      "w^crisis        \t   7975164\t1.00\t1.00\t1.75\t  52.01\n",
      "w^global        \t  14045552\t1.00\t1.00\t1.80\t  53.54\n",
      "w^lgbt          \t   1584965\t1.00\t1.00\t1.81\t  53.85\n",
      "w^support       \t     52402\t1.00\t1.00\t1.88\t  55.69\n",
      "w^burma         \t   9035384\t1.00\t1.00\t1.88\t  55.72\n",
      "w^un            \t   3609549\t1.00\t1.00\t1.91\t  56.76\n",
      "w^activists     \t   8513072\t1.00\t1.00\t1.92\t  56.87\n",
      "w^theccr        \t  10348664\t1.00\t1.00\t1.98\t  58.75\n",
      "w^workers       \t   4582825\t1.00\t1.00\t2.05\t  60.86\n",
      "w^lawsuit       \t   9400854\t1.00\t1.00\t2.07\t  61.52\n",
      "w^justice       \t  12311336\t1.00\t1.00\t2.15\t  63.78\n",
      "w^aclu          \t    350980\t1.00\t1.00\t2.17\t  64.55\n",
      "w^via           \t   3054916\t1.00\t1.00\t2.21\t  65.52\n",
      "w^guantanamo    \t   4256881\t1.00\t1.00\t2.22\t  65.95\n",
      "w^afghanistan   \t     46872\t1.00\t1.00\t2.24\t  66.62\n",
      "w^torture       \t   3271671\t1.00\t1.00\t2.47\t  73.39\n",
      "w^rights        \t  10383972\t1.00\t1.00\t2.51\t  74.41\n",
      "w^wisdomwednesday\t   3624197\t1.00\t1.00\t2.78\t  82.39\n",
      "w^rt            \t  12443308\t1.00\t1.00\t2.79\t  82.87\n",
      "w^humanrights   \t  11464637\t1.00\t1.00\t3.37\t 100.00\n"
     ]
    }
   ],
   "source": [
    "head -50 vw/features\n",
    "tail -50 vw/features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, words with the highest negative weights are often either swear words or Internet slang. For positive words, many make sense, including the actual word humanrights (a hashtag originally), justice, torture, etc. The classifier also seems to have picked up on some countries where there are perhaps more human rights violations, such as Afghanistan, Burma, Iran, etc. Retweets (rt) and sharing stories via an app (via) seem to be much more associated with human rights tweets than non-human rights tweets. Some of the positive word weights seem a bit off, like wisdomwednesday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 2|w wisdomwednesday rt thewjp democracy freedom amp justice dont just happen we must strive for them through action  chen guangcheng \t|l len:130\n",
      "+1 2|w go into the world and do well but more importantly go into the world and do good  minor myers jr wisdomwednesday \t|l len:114\n",
      "+1 2|w wisdomwednesday rt half a society that fails to protect the rights of women is not a free society  laura bush  \t|l len:112\n",
      "+1 2|w wisdomwednesday  \t|l len:18\n",
      "+1 2|w wisdomwednesday there never will be complete equality until women themselves help to make laws and elect lawmakers  susan b anthony \t|l len:133\n",
      "grep: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "grep '.*wisdomwednesday.*' vw/vwtrain.txt | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that one of the human rights accounts used for the training set really enjoyed the hashtag #wisdomwednesday. Looking through the tweets, there seems to be a bigger issue with false positives than with false negatives. There are too many tweets labeled as non-human rights to look through though. Let's look at some of the false negatives that the classifier incorrectly predicted.\n",
    "\n",
    "To be honest, I kind of got sick of using Bash/awk/sed/etc. stuff for everything by this point, so I just wrote a short Python script to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: compare.py [-h] [-r RAWPRED] [-t TEST] [-o OUTPUT]\n",
      "\n",
      "This script takes a file with raw predictions and a labeled set in VW format\n",
      "and outputs a file that contains the misclassified posts. The format is [pred]\n",
      "| [true label] | [rest of post]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -r RAWPRED, --rawpred RAWPRED\n",
      "                        File with the raw predictions\n",
      "  -t TEST, --test TEST  Labeled test set file\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        Where to save mislabeled tweets\n"
     ]
    }
   ],
   "source": [
    "./compare.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "./compare.py -r vw/vwvalid_rawpred.txt -t vw/vwvalid.txt -o vw/vwvalid_mislabeled.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false positives: 195\n",
      "Number of false negatives: 372\n",
      "\n",
      "False positives:\n",
      "+1 | -1 |w after 16 hours qantas a380 from sydney is on the ground  dallasfort worth international airport  \t|l len:98\n",
      "+1 | -1 |w notre dame students there is still time to apply for a fall break trip to guatemala or el salvador with handsorg  \t|l len:115\n",
      "+1 | -1 |w gilcedillocd1 hi cm cedillo would u consider writing a support letter for ethnic studies as grad req in la schools  \t|l len:117\n",
      "+1 | -1 |w mfw wandows support hasnt replied to any of my tweets in months  \t|l len:66\n",
      "+1 | -1 |w did you hear about our new holidayhours we are now open 6 days a week tuessat 10a6p and now sun 12p6p lodi newhours awesomesauce \t|l len:130\n",
      "+1 | -1 |w thanks for covering the windows 10 event at our space for usa today nansanfran  would love to have you back anytime \t|l len:117\n",
      "+1 | -1 |w to achieve victory we must mass our forces at the hub of all power amp movement the enemys center of gravity  carl von clausewitz \t|l len:131\n",
      "+1 | -1 |w rail car specialist training at sertc hosted by cn rail firefighters from across na attending  burlingtonfire  \t|l len:112\n",
      "+1 | -1 |w simple ways to practice kindness  via vinglenet \t|l len:49\n",
      "+1 | -1 |w lawrence thelastword those who were derelict in their duties should get the boot the director should take a firm stand and take action \t|l len:136\n",
      "+1 | -1 |w spartantown ryanwriter sharethis thanks for the writeup glad youre enjoying it the final issue is a doozy  \t|l len:108\n",
      "+1 | -1 |w bitch reach for the door and get your access denied \t|l len:53\n",
      "+1 | -1 |w justice and mercy international gala 2014 full house  \t|l len:55\n",
      "+1 | -1 |w   comando elite la pelicula 2014  via corri2alterados \t|l len:55\n",
      "+1 | -1 |w so inspired right now african literature redux with featured writers    nana ekua brewhammond  \t|l len:96\n"
     ]
    }
   ],
   "source": [
    "echo \"Number of false positives: $(grep '^+1.*' vw/vwvalid_mislabeled.txt | wc -l)\"\n",
    "echo \"Number of false negatives: $(grep '^-1.*' vw/vwvalid_mislabeled.txt | wc -l)\"\n",
    "printf \"\\n\"\n",
    "echo \"False positives:\"\n",
    "grep \"^+1 |\" vw/vwvalid_mislabeled.txt | shuf -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are tweets mistakenly labeled as being related to human rights, when their true label was being not related to human rights. One thing it's picking up on are tweets that thank other people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 2|w conniedineen thanks so much for your support connie \t|l len:53\n",
      "+1 2|w rt unrightswire thank you for following dgd2014  the discussion will resume this afternoon at 3pm cet watch live on  \t|l len:118\n",
      "+1 2|w thank you senrandpaul for supporting meaningful limits to the isil force authorization consistent with our natl security amp rule of law \t|l len:138\n",
      "+1 2|w frenchplums thanks so much for your support paula \t|l len:51\n",
      "+1 2|w check out this great idlonews video  thanks for including our morocco footage a2j womensrights \t|l len:96\n",
      "+1 2|w rt lionshalom thank you hillelneuer you are a legend  they could not hide from the facts   \t|l len:92\n",
      "+1 2|w fhassan15 thanks for your support \t|l len:35\n",
      "+1 2|w rt ijdh thank you to theccr for supporting ijdhs cholera case appeal great article on that here  \t|l len:98\n",
      "+1 2|w rt atomicalandy thanks 530000 people sign to demand naturalfruit drop criminal cases against me  ilrf tucglobal walkfree laboursta \t|l len:132\n",
      "+1 2|w thanks for the rts dwatchnews kohoso and wbengmg \t|l len:50\n",
      "+1 2|w liblbm thanks for linking to us as your charity of the day \t|l len:60\n",
      "+1 2|w my best rts this week came from envirodefenders amerlekelly ecuasoli khrg thanksall who were yours  \t|l len:101\n",
      "+1 2|w my best rts this week came from miningwatch kaskadia envirodefenders nanasilvergrim thanksall who were yours  \t|l len:111\n",
      "+1 2|w un cat senegal this concludes our live tweet answers from senegal tomorrow 11612 at 3 pm  thank you for following us \t|l len:118\n",
      "+1 2|w mikevanderosen thanks for your support michael \t|l len:48\n"
     ]
    }
   ],
   "source": [
    "grep \"^+1 .*thank.*\" vw/vwtrain.txt | shuf -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, one of the artifacts of the way the human rights tweets were collected is that many of the posts now labeled as human rights essentially just say \"thanks for your support\" and don't really have any other words that would indicate being about human rights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 | +1 |w its impossible to not have fun playing in a river riversuniteus  \t|l len:66\n",
      "-1 | +1 |w martinpradel please follow us \t|l len:31\n",
      "-1 | +1 |w rt spmizner mgmudel has it right  beautiful moving and true  nomodernasylum   \t|l len:79\n",
      "-1 | +1 |w class challenges uber fees at lax   \t|l len:37\n",
      "-1 | +1 |w  \t|l len:2\n",
      "-1 | +1 |w when i looked around me i had lost my friends my family my dignity my freedom amp my religion marinanemat cff \t|l len:111\n",
      "-1 | +1 |w the average score for the congressreportcard is 15 is that like an f fail   \t|l len:77\n",
      "-1 | +1 |w overuse safety questions cloud advairs ascent to asthma blockbuster  \t|l len:70\n",
      "-1 | +1 |w  \t|l len:2\n",
      "-1 | +1 |w jadaliyya link is broken \t|l len:26\n",
      "-1 | +1 |w ferrarogiuliano on est daccord \t|l len:32\n",
      "-1 | +1 |w nypalin16 we do see  \t|l len:22\n",
      "-1 | +1 |w man sues walmart over gascan blast   \t|l len:38\n",
      "-1 | +1 |w a few days b4 campaore was ousted ericachenoweth predicted as much based on her research of nonviolent movements  \t|l len:115\n",
      "-1 | +1 |w thepoptort the summer ice cream blues  listeria \t|l len:49\n"
     ]
    }
   ],
   "source": [
    "grep \"^-1 |\" vw/vwvalid_mislabeled.txt | shuf -n 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the \"true\" human rights tweets that were \"mistakenly\" labeled as not being about human rights. Again, some of these labels are questionable. Many of these are clearly considered human rights because it was a human rights account. However, as we can see, human rights accounts don't necessarily give every tweet human rights content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Again\n",
    "\n",
    "There are two simple text pre-processing things and another simple thing that can be done that might improve the classifier.\n",
    "\n",
    "1. __Remove tweets that say thanks__. These being labeled as human rights related are clearly an artifact of the way the labeled data was collected (getting posts from certain accounts associated with human rights). The text itself does not merit the posts' label of being human-rights related. Therefore I will try removing posts that say thanks.\n",
    "\n",
    "2. __Remove the term rt__. Something about the labeled data made it so that the term \"rt\" was more likely to be associated with human rights tweets. Judging from some of the mislabeled tweets, this is generating some of the false positives. I will try removing all instances of the term \"rt\" from the labeled data. \n",
    "\n",
    "3. __Focus on precision__. Looking through some of the mislabeled tweets, it looks like false positives are a greater problem than false negatives. In other words, it may be beneficial to weight precision greater than recall, since it's possible that a poor recall is the result of not predicting the \"human rights tweets\" that say things like \"jadaliyya link is broken\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try the parameters tried in the original random search and get the parameters that maximize the $F_{0.5}$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: .97439686820112401149\n",
      "weight: 2\n",
      "--passes 10 --loss_function logistic --ngram w1 --skips w2 --learning_rate .80000000000000000000\n"
     ]
    }
   ],
   "source": [
    "./bestresult.sh F 0.5 vw/vwresults vw/vw_bestparams_proc\n",
    "cat vw/vw_bestparams_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRF: 0.96789\n",
      "weight: 2\n",
      "--passes 10 --loss_function logistic --ngram w1 --skips w2 --learning_rate .80000000000000000000\n"
     ]
    }
   ],
   "source": [
    "cat vw/vw_bestparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the model that maximized the balanced $F_1$ score maximizes the $F_{0.5}$ score as well. Let's try weighing precision even higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: .97876761676640748976\n",
      "weight: 8\n",
      "--passes 18 --loss_function hinge --ngram w1 --skips w2 --learning_rate .60000000000000000000 --l1 .00001000000000000000\n"
     ]
    }
   ],
   "source": [
    "./bestresult.sh F 0.25 vw/vwresults vw/vw_bestparams_proc\n",
    "cat vw/vw_bestparams_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the $F_{0.25}$ score, now we get something different. VW will weigh human rights examples even higher, perhaps go through more passes in the data, uses hinge loss instead of logistic, has a slightly lower learning rate, and now uses L1 regularization. \n",
    "\n",
    "Let's do our extra text processing steps (remove tweets with the word \"thank*\" and take out the term \"rt\") while using our new weight of 8 instead of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of tweets:99035\n",
      "New number of tweets:96657\n",
      "Original number of tweets:24808\n",
      "New number of tweets:24256\n",
      "Original number of tweets:31114\n",
      "New number of tweets:30379\n"
     ]
    }
   ],
   "source": [
    "labeled=(\"train\" \"valid\" \"test\")\n",
    "\n",
    "for labeledtype in \"${labeled[@]}\";\n",
    "do\n",
    "    cat fast${labeledtype}.txt | sed -e 's/\\<rt\\>//g' > fast${labeledtype}_proc.txt\n",
    "    sed -i '/thank/d' fast${labeledtype}_proc.txt\n",
    "    echo Original number of tweets:$(wc -l < fast${labeledtype}.txt)\n",
    "    echo New number of tweets:$(wc -l < fast${labeledtype}_proc.txt)\n",
    "done\n",
    "\n",
    "files=(\"valid\" \"test\")\n",
    "for data in \"${files[@]}\";\n",
    "do\n",
    "    cat fast${data}_proc.txt | awk '{sub(\"__label__hr\", \"+1 |w\"); sub(\"__label__nonhr\", \"-1 |w\"); print $0}'| shuf > vw/${data}tmpW\n",
    "    cat vw/${data}tmpW | sed 's:.*|w::'| awk '$0=\"|l len:\"length($0)' > vw/${data}tmpL\n",
    "    paste vw/${data}tmpW vw/${data}tmpL > vw/vw${data}_proc.txt\n",
    "    rm vw/${data}tmp*\n",
    "done\n",
    "\n",
    "while read line; do\n",
    "    if [[ $line == weight* ]]; then\n",
    "        weight=$(echo $line | grep -o -P '(?<=weight: )[0-9]+';)\n",
    "    fi\n",
    "    if [[ $line == --* ]]; then\n",
    "        params=$line\n",
    "    fi\n",
    "done < vw/vw_bestparams_proc\n",
    "\n",
    "cat fasttrain_proc.txt | awk '{sub(\"__label__hr\", \"+1 '$weight'|w\"); sub(\"__label__nonhr\", \"-1 |w\"); print $0}' | shuf > vw/traintmpW\n",
    "cat vw/traintmpW | sed 's:.*|w::'| awk '$0=\"|l len:\"length($0)' > vw/traintmpL\n",
    "paste vw/traintmpW vw/traintmpL > vw/vwtrain_proc.txt\n",
    "rm vw/traintmp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing tweets with the word \"thank*\" results in the loss of a few thousand tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--passes 18 --loss_function hinge --ngram w1 --skips w2 --learning_rate .60000000000000000000 --l1 .00001000000000000000\n"
     ]
    }
   ],
   "source": [
    "echo $params\n",
    "vw --binary vw/vwtrain_proc.txt -c -k -f vw/vw_proc.model -b 24 $params --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it performs on the unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw --binary -t -i vw/vw_proc.model -p vw/vwunlabeled_pred_proc.txt vw/vwfirst200k.txt --quiet\n",
    "paste vw/vwunlabeled_pred_proc.txt vw/vwfirst200k.txt > vw/vwlabeled200k_proc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted human rights tweets: 1337\n",
      "Number of total tweets: 211180\n",
      "\n",
      "Predicted as human rights\n",
      "1\t|w forgot how miserable it is to live in a swing state during election season dont complain about your vote not counting dc ondemandonly |l len:134\n",
      "1\t|w isaacmizrahi have an idea gt headscarfsgt charity women wearing in support of kurdish women warriors you  |l len:106\n",
      "1\t|w nigerianewsdesk boko haram lagos court delivers secret judgment on suspected terrorists  via todayngr |l len:102\n",
      "1\t|w because heaven forbid we focus on womens safety for once |l len:57\n",
      "1\t|w the chinese are smarter simply bc they can fit more knowledge into less material a 1000 page book in english would be 500 pages in chinese |l len:139\n",
      "1\t|w markhor14 what an xprnc gr8 speakers proud of youth of pakistan who made this a success full of energy determination jazba passion n luv |l len:137\n",
      "1\t|w update we are arriving at temple university hospital where pa state police will update media on trooper killed in shooting at gun range |l len:136\n",
      "1\t|w hope usa the climate justice alliance   climatejusticealliance |l len:63\n",
      "1\t|w nataliepage15 xavierbaena we need to put them in solitary confinement |l len:70\n",
      "1\t|w insanity is having a democracy and letting one person change a law bc it bothers them majority rulesthus a democracy little lefties |l len:132\n",
      "1\t|w blackrepublican i assume it is part of the pan federal agency collapse of competence |l len:85\n",
      "1\t|w bobbylemaire magicmogo23 less than 10 of native americans care about this issue |l len:80\n",
      "1\t|w privacy privacy my privacy you dont care about privacy you care about alarmism |l len:79\n",
      "1\t|w relocation program coordinator  three pillar plano |l len:51\n",
      "1\t|w amprog antigay laws drive significantly higher rates of poverty for lgbt people   file under no kidding |l len:104\n",
      "1\t|w 1dlatinoa los40colombia la rifa de un perfume  ahre |l len:52\n",
      "1\t|w usc men and women among national attendance leaders |l len:52\n",
      "1\t|w 120k ppl in gaza made homeless by israel just during this most recent attack imagine hamas destroying the home of just 1 israeli |l len:129\n",
      "1\t|w jdfed1975 but this wont prevent the gop from declaring victory amp hypocritically blaming it on the victims of such policies |l len:125\n",
      "1\t|w 1 donation saves 3 lives  the dolan law firm  |l len:46\n",
      "\n",
      "Predicted as not human rights\n",
      "-1\t|w thats one big bowl of stirfried chicken and vegetables  kiyomis bistro  |l len:72\n",
      "-1\t|w not just the big ones either |l len:29\n",
      "-1\t|w broodsmusic  motherandfather  this is our favorite song of this awesome band  check it‚Ä¶  |l len:89\n",
      "-1\t|w foreverpst they pastries |l len:25\n",
      "-1\t|w justineannaw fr like there are 8 freakin days till freakshow and would like to watch coven before it |l len:101\n",
      "-1\t|w wow just freaking wow at this royals offense |l len:45\n",
      "-1\t|w i almost passed out in the shower wtf |l len:38\n",
      "-1\t|w breaclark lmfao xd  i legit forgot |l len:35\n",
      "-1\t|w my work here is doneüëè |l len:22\n",
      "-1\t|w feel free to visit me  werk üòå |l len:30\n",
      "-1\t|w avgplays amazing  swagginjay thacker423 |l len:40\n",
      "-1\t|w stop  |l len:6\n",
      "-1\t|w daonna fr  |l len:11\n",
      "-1\t|w the rocky horror mac collection ‚ù§Ô∏è‚ù§Ô∏è |l len:37\n",
      "-1\t|w ‚Äúeveeekay i need a bottle after the class i just had‚Äùgot you boooooo see ya in five |l len:84\n",
      "-1\t|w annabethsadler im supposed to be the creepy one |l len:48\n",
      "-1\t|w 1dupdatesonline 1detailshq awwwwwww but the shade tho |l len:54\n",
      "-1\t|w top3apps for askbill |l len:21\n",
      "-1\t|w n ag√ºento mais esse lugar |l len:26\n",
      "-1\t|w shutyodumbassup crazy girlfriends be like  |l len:43\n"
     ]
    }
   ],
   "source": [
    "echo Number of predicted human rights tweets: $(grep '^1.*' vw/vwlabeled200k_proc.txt | wc -l)\n",
    "echo Number of total tweets: $(wc -l < vw/vwlabeled200k_proc.txt)\n",
    "printf \"\\n\"\n",
    "echo Predicted as human rights\n",
    "grep '^1.*' vw/vwlabeled200k_proc.txt | shuf -n 20\n",
    "printf \"\\n\"\n",
    "echo Predicted as not human rights\n",
    "grep '^-1.*' vw/vwlabeled200k_proc.txt | shuf -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the above and other tweets, this round seems to have done better than the previous one, though it's still not perfect.\n",
    "\n",
    "Just for kicks, let's see how this model nominally performs on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1-grams for w namespaces.\n",
      "Generating 2-skips for w namespaces.\n",
      "only testing\n",
      "raw predictions = vw/vwtest_rawpred_proc.txt\n",
      "Num weight bits = 24\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = vw/vwtest_proc.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000        9\n",
      "0.000000 0.000000            2            2.0   1.0000   1.0000       17\n",
      "0.000000 0.000000            4            4.0  -1.0000  -1.0000        8\n",
      "0.000000 0.000000            8            8.0  -1.0000  -1.0000        9\n",
      "0.062500 0.125000           16           16.0   1.0000   1.0000       23\n",
      "0.031250 0.000000           32           32.0  -1.0000  -1.0000       11\n",
      "0.046875 0.062500           64           64.0  -1.0000  -1.0000       12\n",
      "0.046875 0.046875          128          128.0  -1.0000  -1.0000        7\n",
      "0.031250 0.015625          256          256.0  -1.0000  -1.0000        7\n",
      "0.035156 0.039062          512          512.0   1.0000   1.0000       14\n",
      "0.029297 0.023438         1024         1024.0  -1.0000  -1.0000        9\n",
      "0.030762 0.032227         2048         2048.0  -1.0000  -1.0000       21\n",
      "0.031738 0.032715         4096         4096.0   1.0000   1.0000       22\n",
      "0.027466 0.023193         8192         8192.0  -1.0000  -1.0000       14\n",
      "0.028137 0.028809        16384        16384.0  -1.0000  -1.0000        9\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 30379\n",
      "passes used = 1\n",
      "weighted example sum = 30379.000000\n",
      "weighted label sum = -9029.000000\n",
      "average loss = 0.027717\n",
      "best constant = -0.297212\n",
      "best constant's loss = 0.911665\n",
      "total feature number = 402608\n",
      "ACC    0.97228   pred_thresh  0.000000\n",
      "PRE    0.97980   pred_thresh  0.000000\n",
      "REC    0.94052   pred_thresh  0.000000\n",
      "PRF    0.95976   pred_thresh  0.000000\n",
      "PRB    0.96384\n"
     ]
    }
   ],
   "source": [
    "vw --binary -t -i vw/vw_proc.model -r vw/vwtest_rawpred_proc.txt vw/vwtest_proc.txt \n",
    "cut -d' ' -f1 vw/vwtest_proc.txt | paste - vw/vwtest_rawpred_proc.txt | perf.linux/perf -t 0 -PRE -REC -PRF -PRB -ACC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model nominally does well. It achieves \n",
    "an accuracy of over 97% and a precision of nearly 0.98, though recall is down at around 0.94."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Next Steps\n",
    "\n",
    "- __More text preprocessing__. Some steps I normally employ that I left out while playing around with VW is removing stopwords (e.g. the, a, ...) and stemming. It's possible that these steps may have helped improve things.\n",
    "\n",
    "- __Manually clean some of the tweets__. To recap, the positive-labeled tweets (tweets considered to be about human rights) were collected from accounts that tweet often about human rights and from posts that used certain hashtags. This strategy is nice because it's presumably low effort and generates a lot of \"labeled\" data rapidly. The downside of this vacuum strategy seems to be that it gets a lot of tweets that would generally be considered unrelated to human rights (see those \"thank you\" tweets). The labeled data could therefore benefit from some manual cleanup and labeling.\n",
    "\n",
    "- __Supplement the training data__. There are 150,000 labeled tweets, with nearly 100,000 of them going to the training set. Given the huge volume of tweets that are produced, this is a very small amount. It's quite likely that the classifier is not being exposed to enough examples of \"regular\" tweets, which can contribute to a feature weighting outcome where many unrelated tweets are labeled as human rights tweets (low precision).\n",
    "\n",
    "- __Force greater regularization__. Models that optimized accuracy or the balanced $F_1$ score tended not to have regularization. However, given what we know about the trustworthiness of the data, optimizing these numbers may not exactly be the way forward; and that we definitely do not want overfitting on this training data. We could therefore run random search (or some other type of search) where all models must have regularization (my current implementation only uses regularization in about half of the random search iterations).\n",
    "\n",
    "- __Take advantage of other VW options__. Some of the options that VW has that I didn't use include boosting and using a multi-layer perceptron with a single hidden layer (with sigmoidal activations I believe). I could try incorporating these, though some initial tests I ran seemed to indicate that they didn't too much to improve things.\n",
    "\n",
    "- __Try different architectures__. A lot of recent advancements in NLP have come through the usage of neural networks. Shallow ones can be used to generate word/sentence/document embeddings. Convolutional neural networks and recurrent neural networks (e.g. LSTMs) can also be used to incorporate aspects of the sentence sequence/structure into learning and predictions. \n",
    "\n",
    "- __Information retrieval__. Rather than treating this as a machine learning problem, we could treat it as a information retrieval problem. This would require us to come up with a set of search terms. I was also asked to do this by training a classifier. Nevertheless, I tried running this data through [Elasticsearch](https://www.elastic.co/products/elasticsearch) with some terms I came up with, and it seemed to do okay\n",
    "![elasticsearch](elasticsearch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Takeaways\n",
    "\n",
    "- Data quality is important....an obvious one. The way the human rights tweets were collected meant that there are potentially a lot of incorrectly applied labels, which is not fantastic for training...\n",
    "\n",
    "- Can't blindly just optimize numbers. I believe the original goal of this project was to use a trained classifier to label tweets as being about human rights or not, and then to map where the human rights tweets were coming from, geographically. If we had been less discerning, we could have observed the high accuracy, $F_1$ score, precision, etc. on our test set, assumed our classifier is doing great, and then directly map the predictions it generated onto the map. As can be seen from our first attempt, and to somewhat lesser extent our second attempt, this would not have been good, since we'd be mapping a lot of incorrect tweets.\n",
    "\n",
    "- Should interrogate the model. We picked up some insights by taking a look through the tweets that were mislabeled and by looking at the features with the highest weights so that the classifier itself would not be a black box. Using a tool like [lime](https://github.com/marcotcr/lime) could have been useful too.\n",
    "\n",
    "- Vowpal Wabbit is fast. I had heard of Facebook's fastText (some code using it is elsewhere in this repo), and I came across VW while reading discussions about fastText. Both tools are nice and speedy, but one advantage VW had over \\[a Python implementation of\\] fastText was its handling of ngrams. Using more than bigrams brought my laptop (4GB RAM) to a crawl, but VW seemed to handle up to 5-grams quite well.\n",
    "\n",
    "- The command line seems kind of unwieldly for these kinds of tasks. The caveat is that I'm more experienced in tools like Python and R relative to command-line things like Bash, awk, sed, etc. I'm glad that I went through this exercise and got a better handle on command line commands though, since I generally didn't use command line stuff besides moving and renaming files and things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
