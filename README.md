# Classifying Human Rights Tweets

This repo is mainly a log of my attempts to help out on a project where the researchers were looking to map the source of tweets abouts human rights in the United States. The training data they used and which was provided to me came from posts made by human rights activists/organizations on Twitter. Both the training/test sets and unlabeled tweets were provided to me with no alteration/supplementation on my part. The main attempt can be found in the Jupyter notebook `vw.ipynb`, which uses [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki) (VW).

## VW
`vw_rsearch.sh` runs random search using VW. The parameters that will vary are 
- The weight given to human rights tweets (1-10)
- The number of passes/epochs through the data (5-20)
- Learning rate (0.1-1)
- The n in n-grams (1-3) (initial testing showed that anything after trigrams seemed to perform quite poorly)
- How many words to skip when working with bigrams or more (0-3)
- Loss type (logistic or hinge)
- Whether to include a quadratic between the text of the tweet and the length
- Presence/strength of regularization (L1, L2, or both/elastic net)

```bash
./vw_rsearch.sh 100 false false vw/vwmodel vw/vwresults fasttrain.txt fastvalid.txt fasttest.txt
```
This tells the random search to run through 100 trials; do not force L1 regularization (to induce sparsity); do not supplement the training data with some of the unlabeled data (to give more negative examples); save the models to `vw/vwmodel`; save the calculated metrics (e.g. accuracy, F1 score, etc.) to `vw/vwresults`; and what files to use for training, validation, and testing. Because these labeled files already existed through my experiments with fastText, this script converts them to VW's default format. That is why the name of the test set file is included, even though it's not touched by the random search itself. Performance metrics are calculated by the [perf](http://osmot.cs.cornell.edu/kddcup/software.html) tool.

`bestresult.sh` takes a the performance metrics generated by `vw_rsearch.sh` and extracts the parameters that optimize the provided metric of choice.

```bash
./bestresult.sh F 0.5 vw/results vw/best_params
```
This tells the script to go through the file `vw/results` to find the parameters that maximize the F0.5 score and write it out to the file `vw/best_params`. Note that I don't think `perf` calculates F scores beyond the balanced F1 score, so this script will calculate the F score given what beta you give it (0.5 in this case).


`compare.py` takes a set of data labeled in VW format and compares it with a file filled with raw predictions by the VW classifier of the same data. Mismatches will be output to another file.

```bash
./compare.py -r vw/vwvalid_rawpred.txt -t vw/vwtest.txt -o vw/vwwrong
```

`removetext.sh` is hardcoded to take the data used for fastText and remove any lines with the word "thank*" in it and to remove the term "rt" from all posts. See `vw.ipynb` for reasoning.

## fastText

I also took a stab at this task using Facebook's [fastText](https://fasttext.cc/). The code to do this is in `hr.py`. Also implements random search where the following parameters vary
- Size of word vectors (50-200)
- Number of epochs/passes through data (1-50)
- Learning rate (0.1 - 1)
- The minimum number of times a word needs to appear before being included (1-10)
- The n in n-grams (1-5)
The script also includes code to generate a confusion matrix (text form), to calculate recall and precision, and to map the tweets.

## Old

The ipython notebook `Chi-Squared.ipynb` in the Old folder was a quick attempt at this task using a logistic regression classifier with chi-squared feature selection and TF-IDF.
